import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
from imblearn.over_sampling import SMOTE
from google.colab import files

# Upload dataset
uploaded = files.upload()
df = pd.read_csv("water_potability.csv")

# Basic info
print(df.head())
print(df.describe())
print(df.info())
print(df.isnull().sum())

# Missing values heatmap
sns.heatmap(df.isnull(), cmap="coolwarm")
plt.show()

# Fill missing values
df.fillna(df.median(), inplace=True)

# Standardize data
scaler = StandardScaler()
X = df.drop("Potability", axis=1)
y = df["Potability"]
X_scaled = scaler.fit_transform(X)

# Correlation heatmap
sns.heatmap(pd.DataFrame(X_scaled, columns=X.columns).corr(), cmap="coolwarm", annot=False)
plt.show()

# PCA
pca = PCA(n_components=6)
X_pca = pca.fit_transform(X_scaled)

# Handle imbalance
smote = SMOTE(sampling_strategy=0.9, random_state=42)
X_res, y_res = smote.fit_resample(X_pca, y)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42, stratify=y_res)

# Models
models = {
    "Logistic Regression": LogisticRegression(),
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(),
    "KNeighbors": KNeighborsClassifier(),
    "SVM": SVC(probability=True),
    "XGBoost": XGBClassifier(eval_metric="logloss")
}

# Train and evaluate
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    print(f"\n{name}")
    print("Accuracy:", accuracy_score(y_test, y_pred))
    print(classification_report(y_test, y_pred))

# Hyperparameter tuning
param_grid = {
    "Random Forest": {"n_estimators": [100, 300, 500], "max_depth": [10, 20, 30]},
    "XGBoost": {"n_estimators": [100, 300, 500], "learning_rate": [0.01, 0.05, 0.1]}
}

tuned = {}
for name, model in models.items():
    if name in param_grid:
        search = RandomizedSearchCV(model, param_grid[name], n_iter=5, cv=3, n_jobs=-1, random_state=42)
        search.fit(X_train, y_train)
        tuned[name] = search.best_estimator_
    else:
        tuned[name] = model

# Best model
best_model_name = max(tuned, key=lambda x: accuracy_score(y_test, tuned[x].predict(X_test)))
best_model = tuned[best_model_name]
y_pred_best = best_model.predict(X_test)

# Confusion matrix
sns.heatmap(confusion_matrix(y_test, y_pred_best), annot=True, fmt='d', cmap="Blues")
plt.title(f"Confusion Matrix - {best_model_name}")
plt.show()

# ROC-AUC score
print(f"ROC-AUC ({best_model_name}):", roc_auc_score(y_test, best_model.predict_proba(X_test)[:,1]))
